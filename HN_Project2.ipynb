{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv file...\n",
      "Getting Traning Data...\n",
      "Getting Testing Data...\n",
      "Reading stop files\n",
      "1. BaseLine with smoothing = 0.5 \n",
      " 2. Stop-word Filtering \n",
      " 3. Word-length Filtering \n",
      " 4. Infrequency word Filtering \n",
      " 5. Changing smoothing values\n",
      "Enter your choice 1-5: 4\n",
      "Frequency  1\n",
      "Building Model with smoothing =  0.5\n",
      "Generate Vocabulary\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  41076\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Performance\n",
      "Accuracy:  [0.931]\n",
      "Precision:  [0.935]\n",
      "Recall:  [0.931]\n",
      "F1-Score:  [0.904]\n",
      "Frequency  5\n",
      "Building Model with smoothing =  0.5\n",
      "Generate Vocabulary\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  18518\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Performance\n",
      "Accuracy:  [0.931, 0.936]\n",
      "Precision:  [0.935, 0.939]\n",
      "Recall:  [0.931, 0.936]\n",
      "F1-Score:  [0.904, 0.914]\n",
      "Frequency  10\n",
      "Building Model with smoothing =  0.5\n",
      "Generate Vocabulary\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  12696\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Performance\n",
      "Accuracy:  [0.931, 0.936, 0.94]\n",
      "Precision:  [0.935, 0.939, 0.942]\n",
      "Recall:  [0.931, 0.936, 0.94]\n",
      "F1-Score:  [0.904, 0.914, 0.92]\n",
      "Frequency  15\n",
      "Building Model with smoothing =  0.5\n",
      "Generate Vocabulary\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  10039\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Performance\n",
      "Accuracy:  [0.931, 0.936, 0.94, 0.941]\n",
      "Precision:  [0.935, 0.939, 0.942, 0.942]\n",
      "Recall:  [0.931, 0.936, 0.94, 0.941]\n",
      "F1-Score:  [0.904, 0.914, 0.92, 0.924]\n",
      "Frequency  20\n",
      "Building Model with smoothing =  0.5\n",
      "Generate Vocabulary\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  8410\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Performance\n",
      "Accuracy:  [0.931, 0.936, 0.94, 0.941, 0.942]\n",
      "Precision:  [0.935, 0.939, 0.942, 0.942, 0.941]\n",
      "Recall:  [0.931, 0.936, 0.94, 0.941, 0.942]\n",
      "F1-Score:  [0.904, 0.914, 0.92, 0.924, 0.926]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import io\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "class trainingData:\n",
    "    NUM_CLASSES = 0\n",
    "    VOCAB_LENGTH = 0\n",
    "    CLASS_TYPES = []\n",
    "    CLASS_VOCAB_LEN = []\n",
    "    CLASS_PROBABILITY = []\n",
    "    ORIGINAL_CLASSIFICATION = []\n",
    "    STOP_WORDS_VOCABULARY = []\n",
    "    PREDICTED_CLASSIFICATION = []\n",
    "    DICT_SCORE = {\"ACCURACY\": [], \"PRECISION\": [], \"RECALL\":\n",
    "        [], \"F1_SCORE\": []}\n",
    "\n",
    "\n",
    "\n",
    "def getVocabulary(trainData, stop_words, word_len_filter,freq_words_filter, frequency):\n",
    "    # Change all words to lowercase\n",
    "    newLowerTitle = trainData[\"Title\"].str.lower()\n",
    "    words = []\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    # Get Tokens\n",
    "    for index, row in newLowerTitle.iteritems():\n",
    "        words.append(tknzr.tokenize(row))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    vocabulary = []\n",
    "\n",
    "    # Lemmatize  words\n",
    "    for word in words:\n",
    "        for w in word:\n",
    "            vocabulary.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "    # Word Length filtering\n",
    "    if word_len_filter:\n",
    "        print('Word length filtering enabled')\n",
    "        vocabulary = [word for word in vocabulary if 2 < len(word) < 9]\n",
    "\n",
    "    # Remove stop words\n",
    "    if stop_words:\n",
    "        print(\"Stop words enabled\")\n",
    "        stop_words = set(line.strip() for line in open('Stopwords.txt'))\n",
    "        vocabulary =[''.join(c for c in s if c not in stop_words) for s in vocabulary]\n",
    "\n",
    "    if freq_words_filter:\n",
    "        freqList = nltk.FreqDist(vocabulary)\n",
    "        vocabulary = [word for word in vocabulary if freqList.get(word)>frequency]\n",
    "        \n",
    "    # Remove Punctuation and special Characters\n",
    "    removePunVocabulary = [''.join(c for c in s if c not in string.punctuation) for s in vocabulary]\n",
    "    removePunVocabulary = [s for s in removePunVocabulary if s]\n",
    "    removePunVocabulary.sort()\n",
    "    removePunVocabulary = list(filter(None, removePunVocabulary))   \n",
    "    \n",
    "    return removePunVocabulary\n",
    "\n",
    "\n",
    "def buildModel(trainData, smooth, stop_words, word_len_filt, freq_words_filter, frequency):\n",
    "    print(\"Building Model with smoothing = \", smooth)\n",
    "    # Generate Vocabulary\n",
    "    print(\"Generate Vocabulary\")\n",
    "    vocabulary = getVocabulary(trainData, stop_words, word_len_filt, freq_words_filter, frequency)\n",
    "    vocabulary = list(filter(None, vocabulary))\n",
    "    counter = collections.Counter(vocabulary)\n",
    "    vocabulary = list(counter.keys())\n",
    "    vocabulary = list(filter(None, vocabulary))\n",
    "\n",
    "    # final sorted vocabulary\n",
    "    # vocabulary.sort()\n",
    "    with io.open('vocabulary.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in vocabulary:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    # Get length of vocabulary\n",
    "    vocabularyLen = len(vocabulary)\n",
    "    trainingData.VOCAB_LENGTH = vocabularyLen\n",
    "    print(\"Complete Vocabulary can be found in vocabulary.txt, vocabulary length = \", vocabularyLen)\n",
    "\n",
    "    # Get number of post types\n",
    "    print(\"Getting number of classes\")\n",
    "    post_type = trainData[\"Post Type\"]\n",
    "    count_type = collections.Counter(post_type)\n",
    "    types = list(count_type.keys())\n",
    "    trainingData.CLASS_TYPES = types\n",
    "    trainingData.NUM_CLASSES = len(types)\n",
    "    print(\"Classes: \", types)\n",
    "    classProbability(post_type, types)\n",
    "\n",
    "    counter = 0\n",
    "    # Get vocabulary set for each post type\n",
    "    print(\"Getting vocabulary of each class and frequency of words\")\n",
    "    for item in types:\n",
    "        sdata = \"Class\" + str(counter) + \"Data\"\n",
    "        globals()[sdata] = trainData[trainData[\"Post Type\"] == item]\n",
    "        svocab = \"Class\" + str(counter) + \"Vocab\"\n",
    "        globals()[svocab] = getVocabulary(globals()[sdata], stop_words, word_len_filt,freq_words_filter, frequency)\n",
    "        globals()[svocab] = list(filter(None, globals()[svocab]))\n",
    "        # length of vocabulary for eah post type\n",
    "        count_type = collections.Counter(globals()[svocab])\n",
    "        sLen = \"Class\" + str(counter) + \"Len\"\n",
    "        globals()[sLen] = len(list(count_type.keys()))\n",
    "        trainingData.CLASS_VOCAB_LEN.append(globals()[sLen])\n",
    "        # frequency of each word in each post type\n",
    "        sFreq = \"Class\" + str(counter) + \"Frequency\"\n",
    "        globals()[sFreq] = nltk.FreqDist(globals()[svocab])\n",
    "        counter = counter + 1\n",
    "\n",
    "\n",
    "\n",
    "    if stop_words:\n",
    "        print(\"Generating stopword-model.txt\")\n",
    "        file = io.open(\"stopword-model.txt\", \"w\", encoding=\"utf-8\")\n",
    "    elif word_len_filt:\n",
    "        print('Generating wordlength-model.txt')\n",
    "        file = io.open(\"wordlength-model.txt\", \"w\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        print(\"Generating model-2018.txt....\")\n",
    "        file = io.open(\"model-2018.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    line = 0\n",
    "    for word in vocabulary:\n",
    "        s = ''\n",
    "        line += 1\n",
    "        # print(line)\n",
    "        s += str(line) + \"  \"\n",
    "        s += word + \"  \"\n",
    "\n",
    "        for i in range(trainingData.NUM_CLASSES):\n",
    "            if globals()[\"Class\" + str(i) + \"Frequency\"].__contains__(word):\n",
    "                c = globals()[\"Class\" + str(i) + \"Frequency\"].get(word)\n",
    "            else:\n",
    "                c = 0\n",
    "            s += str(c) + \"  \"\n",
    "            itemLen = trainingData.CLASS_VOCAB_LEN[i]\n",
    "            probability = (c + smooth) / (itemLen + trainingData.VOCAB_LENGTH * smooth)\n",
    "            s += format(probability, '.12f') + \"  \"\n",
    "        s += '\\r'\n",
    "        file.write(s)\n",
    "    print(\"Complete file \", file.name)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def classProbability(trainClasses, post_types):\n",
    "    classFrequency = nltk.FreqDist(trainClasses)\n",
    "    totalPosts = len(trainClasses)\n",
    "    for item in post_types:\n",
    "        frequency = classFrequency.get(item)\n",
    "        class_prob = frequency / totalPosts\n",
    "        trainingData.CLASS_PROBABILITY.append(class_prob)\n",
    "\n",
    "\n",
    "def buildDictonary(file):\n",
    "    dictionary = {}\n",
    "    model_2018 = file.readlines()\n",
    "    model_2018 = [x.strip() for x in model_2018]\n",
    "    print(\"Building  dict\")\n",
    "\n",
    "    for row in model_2018:\n",
    "        row_split_list = row.split('  ')\n",
    "        dictionary[row_split_list[1]] = row_split_list[2:]\n",
    "\n",
    "    print(\"Created dict \")\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def sum_cond_prob(tokenList, dictionary, smooth, classNum):\n",
    "    sum = 0\n",
    "    # print(\"Calculate conditional probabiliy of words given Class\", classNum)\n",
    "    for token in tokenList:\n",
    "        if token in dictionary:\n",
    "            cond_prob = dictionary.get(token)\n",
    "            index = classNum + classNum + 1\n",
    "            prob = float(cond_prob[index])\n",
    "            sum = sum + math.log10(prob)\n",
    "        else:\n",
    "            prob = smooth / (trainingData.CLASS_VOCAB_LEN[classNum] + trainingData.VOCAB_LENGTH * smooth)\n",
    "            sum += math.log10(prob)\n",
    "    return sum\n",
    "\n",
    "\n",
    "def classify(tokenizeList, dictionary, smooth):\n",
    "    classifed_type = ''\n",
    "    scores = []\n",
    "    for i in range(trainingData.NUM_CLASSES):\n",
    "        probability = math.log10(trainingData.CLASS_PROBABILITY[i]) + sum_cond_prob(tokenizeList, dictionary, smooth, i)\n",
    "        scores.append(probability)\n",
    "    max_prob = max(scores)\n",
    "    for i in range(trainingData.NUM_CLASSES):\n",
    "        if max_prob == scores[i]:\n",
    "            # print(\"max_prob: \",max_prob)\n",
    "            classified_type = trainingData.CLASS_TYPES[i]\n",
    "    d = dict()\n",
    "    d['classType'] = classified_type\n",
    "    d['scores'] = scores\n",
    "    return d\n",
    "\n",
    "\n",
    "# Tokenize tile, Remove puncuations and lemmentize for each title\n",
    "def tokenize_title(title):\n",
    "    # print(\"Tokenizing test data title\")\n",
    "    tweetTokenizer = TweetTokenizer()\n",
    "    tokenList = tweetTokenizer.tokenize(title)\n",
    "    lemmentizeList = []\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    for word in tokenList:\n",
    "        lemmentizeList.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    removePunTokens = [''.join(c for c in s if c not in string.punctuation) for s in lemmentizeList]\n",
    "    removePunTokens = [s for s in removePunTokens if s]\n",
    "    # print(removePunTokens)\n",
    "    return removePunTokens\n",
    "\n",
    "\n",
    "def buildClassifier(testData, smoothing_value, stop_words, word_len_filter, freq_words_filter, frequency):\n",
    "    if stop_words:\n",
    "        file = io.open(\"stopword-model.txt\", encoding=\"utf-8\")\n",
    "        dictionary = buildDictonary(file)\n",
    "        print(\"Generating stopword-result.txt...\")\n",
    "        file = io.open(\"stopword-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "    elif word_len_filter:\n",
    "        file = io.open(\"wordlength-model.txt\", encoding=\"utf-8\")\n",
    "        dictionary = buildDictonary(file)\n",
    "        print(\"Generating wordlength-result.txt...\")\n",
    "        file = io.open(\"wordlength-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        file = io.open(\"model-2018.txt\", encoding=\"utf-8\")\n",
    "        dictionary = buildDictonary(file)\n",
    "        print(\"Generating baseline-result.txt....\")\n",
    "        file = io.open(\"baseline-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "    line = 0\n",
    "    for index, row in testData.iterrows():\n",
    "        label = \"right\";\n",
    "        s = ''\n",
    "        line += 1\n",
    "        s += str(line) + \"  \"\n",
    "        s += row[\"Title\"] + \"  \"\n",
    "        tokenizeList = tokenize_title(row[\"Title\"])\n",
    "        original_post_type = row[\"Post Type\"]\n",
    "        trainingData.ORIGINAL_CLASSIFICATION.append(original_post_type)\n",
    "        # print(\"Reading the contents of the model file\")\n",
    "        dList = classify(tokenizeList, dictionary, smoothing_value)\n",
    "        classification = dList.get('classType')\n",
    "        trainingData.PREDICTED_CLASSIFICATION.append(classification)\n",
    "        class_prob = dList.get('scores')\n",
    "        # print(line, \"original: \", original_post_type, \"classifier: \",classification)\n",
    "        if classification != original_post_type:\n",
    "            label = \"wrong\"\n",
    "        s += classification + \"  \"\n",
    "        for i in range(trainingData.NUM_CLASSES):\n",
    "            s += str(class_prob[i]) + \"  \"\n",
    "        s += label + \"  \"\n",
    "        s += '\\r'\n",
    "        file.write(s)\n",
    "    print(\"Completed\", file.name)\n",
    "\n",
    "\n",
    "def calculatebaseperformance():\n",
    "    target_names = trainingData.CLASS_TYPES\n",
    "    classification_report = metrics.classification_report(trainingData.ORIGINAL_CLASSIFICATION, trainingData.\n",
    "                                                          PREDICTED_CLASSIFICATION, target_names)\n",
    "    f1_score = metrics.f1_score(trainingData.PREDICTED_CLASSIFICATION, trainingData.ORIGINAL_CLASSIFICATION,\n",
    "                                target_names, average='weighted')\n",
    "    precision = metrics.precision_score(trainingData.PREDICTED_CLASSIFICATION, trainingData.ORIGINAL_CLASSIFICATION,\n",
    "                                        target_names, average='weighted')\n",
    "    print(\"F1 Score\", f1_score)\n",
    "    print(\"Precision\", precision)\n",
    "    print(metrics.classification_report(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                        target_names))\n",
    "\n",
    "def calculatePerformance(value):\n",
    "    print(\"Calculating Performance\")\n",
    "    accuracy_score = metrics.accuracy_score(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                            trainingData.CLASS_TYPES)\n",
    "    precision = metrics.precision_score(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                        trainingData.CLASS_TYPES, average='weighted')\n",
    "    recall = metrics.recall_score(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                  trainingData.CLASS_TYPES, average='weighted')\n",
    "    f1_score = metrics.f1_score(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                trainingData.CLASS_TYPES, average='weighted')\n",
    "    accuracy_list = trainingData.DICT_SCORE.get(\"ACCURACY\")\n",
    "    precision_list = trainingData.DICT_SCORE.get(\"PRECISION\")\n",
    "    recall_list = trainingData.DICT_SCORE.get(\"RECALL\")\n",
    "    f1_score_list = trainingData.DICT_SCORE.get(\"F1_SCORE\")\n",
    "\n",
    "    accuracy_list.append(round(accuracy_score, 3))\n",
    "    precision_list.append(round(precision, 3))\n",
    "    recall_list.append(round(recall, 3))\n",
    "    f1_score_list.append(round(f1_score, 3))\n",
    "    \n",
    "\n",
    "\n",
    "def plotPerformace():\n",
    "    smoothing_list = [x / 10 for x in range(1, 11)]\n",
    "    accuracy_list = trainingData.DICT_SCORE.get(\"ACCURACY\")\n",
    "    precision_list = trainingData.DICT_SCORE.get(\"PRECISION\")\n",
    "    recall_list = trainingData.DICT_SCORE.get(\"RECALL\")\n",
    "    f1_score_list = trainingData.DICT_SCORE.get(\"F1_SCORE\")\n",
    "\n",
    "    df = pd.DataFrame(np.c_[accuracy_list, precision_list, recall_list, f1_score_list], index=smoothing_list)\n",
    "    ax = df.plot.bar()\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    ax.legend([\"ACCURACY\", \"PRECISION\", \"RECALL\", \"F1_SCORE\"])\n",
    "    ax.set_xlabel(\"Smoothing values\")\n",
    "    ax.set_ylabel(\"Performance \")\n",
    "    plt.show()\n",
    "    \n",
    "def plotFreqPerformace():\n",
    "    fRange = np.linspace(1,20,5)\n",
    "    Range = [int(i) for i in fRange]\n",
    "    accuracy_list = trainingData.DICT_SCORE.get(\"ACCURACY\")\n",
    "    precision_list = trainingData.DICT_SCORE.get(\"PRECISION\")\n",
    "    recall_list = trainingData.DICT_SCORE.get(\"RECALL\")\n",
    "    f1_score_list = trainingData.DICT_SCORE.get(\"F1_SCORE\")\n",
    "\n",
    "    df = pd.DataFrame(np.c_[accuracy_list, precision_list, recall_list, f1_score_list], index=Range)\n",
    "    ax = df.plot.bar()\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    ax.legend([\"ACCURACY\", \"PRECISION\", \"RECALL\", \"F1_SCORE\"])\n",
    "    ax.set_xlabel(\"Frequency values\")\n",
    "    ax.set_ylabel(\"Performance \")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Read csv File\n",
    "    print(\"Reading csv file...\")\n",
    "    dataFrame = pd.read_csv(\"hn2018_2019.csv\")\n",
    "    dataFrame[\"Created At\"] = pd.to_datetime(dataFrame[\"Created At\"])\n",
    "    dataFrame[\"year\"] = dataFrame[\"Created At\"].dt.year\n",
    "    dataFrame[\"Title\"] = dataFrame[\"Title\"].str.lower()\n",
    "    # Get data frames from 2018 and 2019\n",
    "    print(\"Getting Traning Data...\")\n",
    "    trainData = dataFrame[dataFrame[\"year\"] == 2018]\n",
    "    print(\"Getting Testing Data...\")\n",
    "    testData = dataFrame[dataFrame[\"year\"] == 2019]\n",
    "    # Reading stop words\n",
    "    print('Reading stop files')\n",
    "    file = open('StopWords.txt', 'r')\n",
    "    trainingData.STOP_WORDS_VOCABULARY = file.read().splitlines()\n",
    "    \n",
    "    print(\"1. BaseLine with smoothing = 0.5 \\n 2. Stop-word Filtering \\n 3. Word-length Filtering \\n 4. Infrequency word Filtering \\n 5. Changing smoothing values\")\n",
    "    choice_str = input(\"Enter your choice 1-5: \")\n",
    "    choice = int(choice_str)\n",
    "    if choice == 1:\n",
    "        stop_words = False\n",
    "        word_len_filt = False\n",
    "        freq_words_filter = False\n",
    "        smoothingvalue = 0.5\n",
    "        frequency = -1\n",
    "        buildModel(trainData, smoothingvalue, stop_words,word_len_filt,freq_words_filter, frequency)\n",
    "        buildClassifier(testData, smoothingvalue, stop_words,word_len_filt, freq_words_filter,frequency)\n",
    "    \n",
    "    elif choice == 2:\n",
    "        stop_words = True\n",
    "        word_len_filt = False\n",
    "        freq_words_filter = False\n",
    "        smoothingvalue = 0.5\n",
    "        frequency = -1\n",
    "        buildModel(trainData, smoothingvalue, stop_words,word_len_filt,freq_words_filter, frequency)\n",
    "        buildClassifier(testData, smoothingvalue, stop_words,word_len_filt, freq_words_filter, frequency)\n",
    "    \n",
    "    elif choice == 3:\n",
    "        stop_words = False\n",
    "        word_len_filt = True\n",
    "        freq_words_filter = False\n",
    "        smoothingvalue = 0.5\n",
    "        frequency = -1\n",
    "        buildModel(trainData, smoothingvalue, stop_words,word_len_filt,freq_words_filter, frequency)\n",
    "        buildClassifier(testData, smoothingvalue, stop_words,word_len_filt, freq_words_filter, frequency)\n",
    "        \n",
    "    elif choice == 4:\n",
    "        stop_words = False\n",
    "        word_len_filt = False\n",
    "        freq_words_filter = True\n",
    "        smoothingvalue = 0.5\n",
    "        fRange = np.linspace(1,20,5)\n",
    "        Range = [int(i) for i in fRange]\n",
    "        for frequency in Range:\n",
    "            print('Frequency ', frequency)\n",
    "            buildModel(trainData, smoothingvalue, stop_words,word_len_filt,freq_words_filter, frequency)\n",
    "            buildClassifier(testData, smoothingvalue, stop_words,word_len_filt, freq_words_filter, frequency)\n",
    "            calculatePerformance(frequency)\n",
    "        plotFreqPerformace()\n",
    "    \n",
    "    elif choice == 5:\n",
    "        stop_words = False\n",
    "        word_len_filt = False\n",
    "        freq_words_filter = False\n",
    "        frequency = -1\n",
    "        smoothing_value_range = [x / 10 for x in range(1, 11)]\n",
    "        for smoothingvalue in smoothing_value_range:\n",
    "            print('Smoothing value ', smoothingvalue)\n",
    "            buildModel(trainData, smoothingvalue, stop_words,word_len_filt,freq_words_filter, frequency)\n",
    "            buildClassifier(testData, smoothingvalue, stop_words,word_len_filt, freq_words_filter, frequency)\n",
    "            calculatePerformance(smoothingvalue)\n",
    "        plotPerformace()\n",
    " \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
